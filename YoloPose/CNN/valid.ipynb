{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 135.7ms\n",
      "Speed: 4.0ms preprocess, 135.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step\n",
      "\n",
      "0: 384x640 1 person, 98.9ms\n",
      "Speed: 3.0ms preprocess, 98.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\n",
      "0: 384x640 1 person, 95.6ms\n",
      "Speed: 3.0ms preprocess, 95.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\n",
      "0: 384x640 1 person, 97.9ms\n",
      "Speed: 3.0ms preprocess, 97.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\n",
      "0: 384x640 1 person, 101.0ms\n",
      "Speed: 3.0ms preprocess, 101.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\n",
      "0: 384x640 1 person, 90.4ms\n",
      "Speed: 2.0ms preprocess, 90.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\n",
      "0: 384x640 1 person, 103.3ms\n",
      "Speed: 3.0ms preprocess, 103.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\n",
      "0: 384x640 1 person, 113.3ms\n",
      "Speed: 3.0ms preprocess, 113.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\n",
      "0: 384x640 1 person, 100.0ms\n",
      "Speed: 2.5ms preprocess, 100.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\n",
      "0: 384x640 1 person, 97.0ms\n",
      "Speed: 4.0ms preprocess, 97.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\n",
      "0: 384x640 1 person, 95.4ms\n",
      "Speed: 2.5ms preprocess, 95.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\n",
      "0: 384x640 1 person, 102.0ms\n",
      "Speed: 3.0ms preprocess, 102.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\n",
      "0: 384x640 1 person, 97.7ms\n",
      "Speed: 3.0ms preprocess, 97.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\n",
      "0: 384x640 1 person, 104.9ms\n",
      "Speed: 3.5ms preprocess, 104.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\n",
      "0: 384x640 1 person, 95.4ms\n",
      "Speed: 2.5ms preprocess, 95.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\n",
      "0: 384x640 1 person, 97.4ms\n",
      "Speed: 2.5ms preprocess, 97.4ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\n",
      "0: 384x640 1 person, 88.0ms\n",
      "Speed: 3.0ms preprocess, 88.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\n",
      "0: 384x640 1 person, 100.0ms\n",
      "Speed: 3.0ms preprocess, 100.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\n",
      "0: 384x640 1 person, 109.9ms\n",
      "Speed: 3.5ms preprocess, 109.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\n",
      "0: 384x640 1 person, 101.8ms\n",
      "Speed: 3.5ms preprocess, 101.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\n",
      "0: 384x640 1 person, 96.9ms\n",
      "Speed: 3.0ms preprocess, 96.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\n",
      "0: 384x640 1 person, 93.2ms\n",
      "Speed: 4.0ms preprocess, 93.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\n",
      "0: 384x640 1 person, 92.9ms\n",
      "Speed: 2.0ms preprocess, 92.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\n",
      "0: 384x640 1 person, 106.7ms\n",
      "Speed: 3.5ms preprocess, 106.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\n",
      "0: 384x640 1 person, 100.9ms\n",
      "Speed: 3.0ms preprocess, 100.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\n",
      "0: 384x640 1 person, 91.5ms\n",
      "Speed: 3.0ms preprocess, 91.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\n",
      "0: 384x640 1 person, 93.6ms\n",
      "Speed: 3.0ms preprocess, 93.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\n",
      "0: 384x640 1 person, 96.2ms\n",
      "Speed: 3.0ms preprocess, 96.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\n",
      "0: 384x640 1 person, 100.0ms\n",
      "Speed: 2.5ms preprocess, 100.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\n",
      "0: 384x640 1 person, 100.0ms\n",
      "Speed: 3.0ms preprocess, 100.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\n",
      "0: 384x640 1 person, 90.5ms\n",
      "Speed: 3.0ms preprocess, 90.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\n",
      "0: 384x640 1 person, 94.4ms\n",
      "Speed: 2.5ms preprocess, 94.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\n",
      "0: 384x640 1 person, 93.4ms\n",
      "Speed: 3.0ms preprocess, 93.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\n",
      "0: 384x640 1 person, 98.0ms\n",
      "Speed: 3.5ms preprocess, 98.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\n",
      "0: 384x640 1 person, 99.4ms\n",
      "Speed: 3.5ms preprocess, 99.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\n",
      "0: 384x640 1 person, 93.2ms\n",
      "Speed: 3.0ms preprocess, 93.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\n",
      "0: 384x640 1 person, 94.0ms\n",
      "Speed: 3.6ms preprocess, 94.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\n",
      "0: 384x640 1 person, 93.4ms\n",
      "Speed: 4.0ms preprocess, 93.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\n",
      "0: 384x640 1 person, 94.9ms\n",
      "Speed: 3.0ms preprocess, 94.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\n",
      "0: 384x640 1 person, 106.0ms\n",
      "Speed: 3.5ms preprocess, 106.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\n",
      "0: 384x640 1 person, 121.3ms\n",
      "Speed: 3.6ms preprocess, 121.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\n",
      "0: 384x640 1 person, 93.0ms\n",
      "Speed: 3.0ms preprocess, 93.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\n",
      "0: 384x640 1 person, 92.4ms\n",
      "Speed: 2.6ms preprocess, 92.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\n",
      "0: 384x640 1 person, 99.0ms\n",
      "Speed: 3.0ms preprocess, 99.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\n",
      "0: 384x640 1 person, 102.5ms\n",
      "Speed: 3.6ms preprocess, 102.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\n",
      "0: 384x640 1 person, 97.8ms\n",
      "Speed: 3.6ms preprocess, 97.8ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\n",
      "0: 384x640 1 person, 98.7ms\n",
      "Speed: 2.5ms preprocess, 98.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\n",
      "0: 384x640 1 person, 103.1ms\n",
      "Speed: 3.0ms preprocess, 103.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\n",
      "0: 384x640 1 person, 102.2ms\n",
      "Speed: 3.5ms preprocess, 102.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\n",
      "0: 384x640 1 person, 104.4ms\n",
      "Speed: 3.0ms preprocess, 104.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\n",
      "0: 384x640 1 person, 101.1ms\n",
      "Speed: 3.5ms preprocess, 101.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\n",
      "0: 384x640 1 person, 93.9ms\n",
      "Speed: 3.5ms preprocess, 93.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\n",
      "0: 384x640 1 person, 93.4ms\n",
      "Speed: 3.0ms preprocess, 93.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\n",
      "0: 384x640 1 person, 96.4ms\n",
      "Speed: 2.5ms preprocess, 96.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\n",
      "0: 384x640 2 persons, 95.4ms\n",
      "Speed: 3.0ms preprocess, 95.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\n",
      "0: 384x640 1 person, 95.8ms\n",
      "Speed: 3.5ms preprocess, 95.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\n",
      "0: 384x640 1 person, 100.1ms\n",
      "Speed: 2.5ms preprocess, 100.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\n",
      "0: 384x640 1 person, 97.9ms\n",
      "Speed: 2.5ms preprocess, 97.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\n",
      "0: 384x640 1 person, 93.9ms\n",
      "Speed: 3.0ms preprocess, 93.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\n",
      "0: 384x640 1 person, 104.2ms\n",
      "Speed: 2.0ms preprocess, 104.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\n",
      "0: 384x640 1 person, 98.5ms\n",
      "Speed: 3.0ms preprocess, 98.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\n",
      "0: 384x640 1 person, 94.0ms\n",
      "Speed: 3.0ms preprocess, 94.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\n",
      "0: 384x640 1 person, 93.6ms\n",
      "Speed: 3.5ms preprocess, 93.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\n",
      "0: 384x640 1 person, 93.4ms\n",
      "Speed: 2.0ms preprocess, 93.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\n",
      "0: 384x640 1 person, 89.9ms\n",
      "Speed: 3.5ms preprocess, 89.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\n",
      "0: 384x640 1 person, 98.4ms\n",
      "Speed: 2.0ms preprocess, 98.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\n",
      "0: 384x640 1 person, 101.5ms\n",
      "Speed: 2.5ms preprocess, 101.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\n",
      "0: 384x640 1 person, 98.4ms\n",
      "Speed: 3.5ms preprocess, 98.4ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\n",
      "0: 384x640 1 person, 93.0ms\n",
      "Speed: 3.0ms preprocess, 93.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\n",
      "0: 384x640 1 person, 97.8ms\n",
      "Speed: 2.0ms preprocess, 97.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\n",
      "0: 384x640 1 person, 118.1ms\n",
      "Speed: 3.0ms preprocess, 118.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\n",
      "0: 384x640 1 person, 104.7ms\n",
      "Speed: 3.0ms preprocess, 104.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\n",
      "0: 384x640 1 person, 94.6ms\n",
      "Speed: 3.0ms preprocess, 94.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\n",
      "0: 384x640 1 person, 116.1ms\n",
      "Speed: 3.0ms preprocess, 116.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\n",
      "0: 384x640 1 person, 106.7ms\n",
      "Speed: 3.0ms preprocess, 106.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\n",
      "0: 384x640 1 person, 207.5ms\n",
      "Speed: 17.1ms preprocess, 207.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\n",
      "0: 384x640 1 person, 149.6ms\n",
      "Speed: 5.6ms preprocess, 149.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\n",
      "0: 384x640 1 person, 131.6ms\n",
      "Speed: 4.0ms preprocess, 131.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\n",
      "0: 384x640 1 person, 111.5ms\n",
      "Speed: 3.5ms preprocess, 111.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\n",
      "0: 384x640 1 person, 101.9ms\n",
      "Speed: 2.0ms preprocess, 101.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\n",
      "0: 384x640 1 person, 99.5ms\n",
      "Speed: 3.5ms preprocess, 99.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\n",
      "0: 384x640 1 person, 112.7ms\n",
      "Speed: 3.5ms preprocess, 112.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\n",
      "0: 384x640 1 person, 109.1ms\n",
      "Speed: 2.0ms preprocess, 109.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\n",
      "0: 384x640 1 person, 117.4ms\n",
      "Speed: 2.5ms preprocess, 117.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\n",
      "0: 384x640 1 person, 111.6ms\n",
      "Speed: 4.0ms preprocess, 111.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\n",
      "0: 384x640 1 person, 100.1ms\n",
      "Speed: 3.0ms preprocess, 100.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\n",
      "0: 384x640 1 person, 105.9ms\n",
      "Speed: 3.5ms preprocess, 105.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\n",
      "0: 384x640 1 person, 103.6ms\n",
      "Speed: 2.0ms preprocess, 103.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\n",
      "0: 384x640 1 person, 89.4ms\n",
      "Speed: 3.5ms preprocess, 89.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\n",
      "0: 384x640 1 person, 98.4ms\n",
      "Speed: 3.5ms preprocess, 98.4ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\n",
      "0: 384x640 1 person, 86.4ms\n",
      "Speed: 2.0ms preprocess, 86.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\n",
      "0: 384x640 1 person, 130.0ms\n",
      "Speed: 2.0ms preprocess, 130.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\n",
      "0: 384x640 1 person, 99.0ms\n",
      "Speed: 3.0ms preprocess, 99.0ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\n",
      "0: 384x640 1 person, 93.5ms\n",
      "Speed: 3.5ms preprocess, 93.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\n",
      "0: 384x640 1 person, 94.4ms\n",
      "Speed: 2.5ms preprocess, 94.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\n",
      "0: 384x640 1 person, 88.4ms\n",
      "Speed: 3.5ms preprocess, 88.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\n",
      "0: 384x640 1 person, 98.9ms\n",
      "Speed: 2.5ms preprocess, 98.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\n",
      "0: 384x640 1 person, 98.5ms\n",
      "Speed: 2.5ms preprocess, 98.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\n",
      "0: 384x640 1 person, 93.9ms\n",
      "Speed: 3.0ms preprocess, 93.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\n",
      "0: 384x640 1 person, 93.5ms\n",
      "Speed: 3.0ms preprocess, 93.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\n",
      "0: 384x640 1 person, 98.5ms\n",
      "Speed: 3.5ms preprocess, 98.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\n",
      "0: 384x640 1 person, 93.4ms\n",
      "Speed: 2.0ms preprocess, 93.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import load_model\n",
    "from ultralytics import YOLO\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "\n",
    "def smooth_data(data, window_size=5, polyorder=2):\n",
    "    if len(data) < window_size:\n",
    "        return data  # No suficiente data para suavizar\n",
    "    return savgol_filter(data, window_size, polyorder, axis=0)\n",
    "\n",
    "def calculate_angle(a, b, c):\n",
    "    a, b, c = np.array(a), np.array(b), np.array(c)\n",
    "    ba = a - b\n",
    "    bc = c - b\n",
    "    cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n",
    "    angle = np.arccos(np.clip(cosine_angle, -1.0, 1.0))\n",
    "    return np.degrees(angle)\n",
    "\n",
    "def calculate_distance(a, b):\n",
    "    return np.linalg.norm(np.array(a) - np.array(b))\n",
    "\n",
    "def extract_pose_features(video_path, model, scaler_X, encoder, modelYolo):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        alto_original, ancho_original = frame.shape[:2]\n",
    "        nuevo_alto = 1100  # Nueva altura deseada\n",
    "        nuevo_ancho = int((nuevo_alto / alto_original) * ancho_original)\n",
    "        frame = cv2.resize(frame, (nuevo_ancho, nuevo_alto))\n",
    "\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = modelYolo(frame)\n",
    "\n",
    "        keypoints = results[0].keypoints.data.numpy().tolist()[0]\n",
    "        \n",
    "        # Definir las conexiones entre keypoints\n",
    "        connections = [(5, 6), (5, 7), (6, 8), (7, 9), (8, 10), (11, 12), (5, 11), (6, 12), (11, 13), (12, 14), (13, 15), (14, 16)]\n",
    "\n",
    "        for connection in connections:\n",
    "            start_keypoint = connection[0]\n",
    "            end_keypoint = connection[1]\n",
    "\n",
    "            # Obtener coordenadas del punto inicial y final de la conexión\n",
    "            start_point = (int(keypoints[start_keypoint][0]), int(keypoints[start_keypoint][1]))\n",
    "            end_point = (int(keypoints[end_keypoint][0]), int(keypoints[end_keypoint][1]))\n",
    "\n",
    "            # Dibujar línea entre los puntos\n",
    "            cv2.line(frame, start_point, end_point, (255, 0, 0), 2)\n",
    "\n",
    "            # Dibujar puntos de inicio y fin de la conexión\n",
    "            cv2.circle(frame, start_point, 5, (0, 255, 0), -1)\n",
    "            cv2.circle(frame, end_point, 5, (0, 255, 0), -1)\n",
    "\n",
    "        if len(keypoints) == 17:\n",
    "\n",
    "\n",
    "            # Extract relevant angles and distances\n",
    "            left_shoulder = keypoints[5]\n",
    "            right_shoulder = keypoints[6]\n",
    "            left_elbow = keypoints[7]\n",
    "            right_elbow = keypoints[8]\n",
    "            left_wrist = keypoints[9]\n",
    "            right_wrist = keypoints[10]\n",
    "            left_hip = keypoints[11]\n",
    "            right_hip = keypoints[12]\n",
    "            left_knee = keypoints[13]\n",
    "            right_knee = keypoints[14]\n",
    "            left_ankle = keypoints[15]\n",
    "            right_ankle = keypoints[16]\n",
    "\n",
    "            # Example angles\n",
    "            left_elbow_angle = calculate_angle(left_shoulder, left_elbow, left_wrist)\n",
    "            right_elbow_angle = calculate_angle(right_shoulder, right_elbow, right_wrist)\n",
    "            left_shoulder_angle = calculate_angle(left_hip, left_shoulder, left_elbow)\n",
    "            right_shoulder_angle = calculate_angle(right_hip, right_shoulder, right_elbow)\n",
    "            left_knee_angle = calculate_angle(left_hip, left_knee, left_ankle)\n",
    "            right_knee_angle = calculate_angle(right_hip, right_knee, right_ankle)\n",
    "            \n",
    "            # Example distances\n",
    "            wrist_distance = calculate_distance(left_wrist, right_wrist)\n",
    "            ankle_distance = calculate_distance(left_ankle, right_ankle)\n",
    "            shoulder_distance = calculate_distance(left_shoulder, right_shoulder)\n",
    "            hip_distance = calculate_distance(left_hip, right_hip)\n",
    "\n",
    "            features = np.array(keypoints).flatten().tolist()\n",
    "\n",
    "            X = scaler_X.transform(np.array(features).reshape(1, -1))\n",
    "            X = smooth_data(X)\n",
    "\n",
    "            predictions = model.predict(X)\n",
    "\n",
    "            # Separar las predicciones\n",
    "            predicciones_continuas = predictions[0][0]\n",
    "            prediccion_clase = encoder.inverse_transform(np.argmax(predictions[1], axis=1))\n",
    "\n",
    "            features_video = [left_elbow_angle, right_elbow_angle, left_shoulder_angle, right_shoulder_angle, left_knee_angle, right_knee_angle, wrist_distance, ankle_distance, shoulder_distance, hip_distance]\n",
    "\n",
    "\n",
    "            # Comparar características del video con las predichas\n",
    "            porcentaje_diferencia = 0.2 \n",
    "            caracteristicas = [\"Angulo del codo izquierdo\", \"Angulo del codo derecho\", \"Angulo del hombro izquierdo\", \"Angulo del hombro derecho\", \"Angulo de la rodilla izquierda\", \"Angulo de la rodilla derecha\", \"Distancia entre munecas\", \"Distancia entre tobillos\", \"Distancia entre hombros\", \"Distancia entre caderas\"]\n",
    "\n",
    "\n",
    "            array1 = np.array(predicciones_continuas)\n",
    "            array2 = np.array(smooth_data(features_video))\n",
    "\n",
    "            relacion = np.abs(1 - (array1 / array2))\n",
    "\n",
    "            errores = [caracteristicas[i] for i in range(len(relacion)) if porcentaje_diferencia< relacion[i] < (1-porcentaje_diferencia)]\n",
    "\n",
    "\n",
    "            clase_predicha = prediccion_clase[0]\n",
    "            porcentaje_prediccion = np.max(predictions[1]) * 100\n",
    "\n",
    "            # Mostrar la predicción de clase\n",
    "            texto_clasificacion = f'Clasificacion: {clase_predicha} - Probabilidad: {porcentaje_prediccion:.2f}%'\n",
    "            cv2.putText(frame, texto_clasificacion, (50, 200), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "\n",
    "            if len(errores) > 0:\n",
    "                for i,error in enumerate(errores):\n",
    "                    desplazamiento_vertical = 240 + (i * 30)  # Incrementa la posición vertical para cada error adicional\n",
    "                    cv2.putText(frame, f'Error en : {error}', (50, desplazamiento_vertical), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "\n",
    "\n",
    "        cv2.imshow('Frame', cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "        # Listen for 'q' key to quit\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "# Cargar el modelo y las clases\n",
    "ruta_actual = os.getcwd()\n",
    "carpeta_Yolo= os.path.dirname(ruta_actual)\n",
    "ruta_raiz = os.path.dirname(carpeta_Yolo)\n",
    "\n",
    "ruta_dataset = os.path.join(ruta_raiz, 'Data Set') \n",
    "ruta_video = os.path.join(ruta_dataset, 'test_video_4.mp4')\n",
    "ruta_npy = os.path.join(ruta_actual, 'datos', 'classes.npy')\n",
    "ruta_scaler = os.path.join(ruta_actual,'datos','scaler_X.pkl')\n",
    "ruta_modelo = os.path.join(ruta_actual, 'model', 'exercise_model.h5')\n",
    "\n",
    "modelYolo = YOLO(\"yolov8n-pose.pt\")\n",
    "\n",
    "# Cargar el modelo y scaler\n",
    "model = load_model(ruta_modelo)\n",
    "scaler_X = joblib.load(ruta_scaler)\n",
    "\n",
    "classes = np.load(ruta_npy, allow_pickle=True)\n",
    "encoder = LabelEncoder()\n",
    "encoder.classes_ = classes\n",
    "\n",
    "# Realizar la predicción\n",
    "extract_pose_features(ruta_video,model,scaler_X,encoder,modelYolo)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Unir_TFM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

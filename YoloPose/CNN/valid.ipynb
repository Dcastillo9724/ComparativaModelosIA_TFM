{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8n-pose.pt to 'yolov8n-pose.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6.51M/6.51M [00:00<00:00, 14.0MB/s]\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x352 1 person, 204.1ms\n",
      "Speed: 4.0ms preprocess, 204.1ms inference, 1248.5ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step\n",
      "\n",
      "0: 640x352 1 person, 79.6ms\n",
      "Speed: 3.0ms preprocess, 79.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\n",
      "0: 640x352 1 person, 102.6ms\n",
      "Speed: 2.0ms preprocess, 102.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\n",
      "0: 640x352 1 person, 107.1ms\n",
      "Speed: 3.0ms preprocess, 107.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\n",
      "0: 640x352 1 person, 102.6ms\n",
      "Speed: 2.0ms preprocess, 102.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\n",
      "0: 640x352 1 person, 99.5ms\n",
      "Speed: 3.0ms preprocess, 99.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\n",
      "0: 640x352 1 person, 92.5ms\n",
      "Speed: 3.0ms preprocess, 92.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\n",
      "0: 640x352 1 person, 97.5ms\n",
      "Speed: 2.0ms preprocess, 97.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\n",
      "0: 640x352 1 person, 85.6ms\n",
      "Speed: 3.0ms preprocess, 85.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\n",
      "0: 640x352 1 person, 85.0ms\n",
      "Speed: 2.0ms preprocess, 85.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\n",
      "0: 640x352 1 person, 83.5ms\n",
      "Speed: 2.0ms preprocess, 83.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\n",
      "0: 640x352 1 person, 85.6ms\n",
      "Speed: 2.0ms preprocess, 85.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\n",
      "0: 640x352 1 person, 102.5ms\n",
      "Speed: 2.0ms preprocess, 102.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\n",
      "0: 640x352 1 person, 92.6ms\n",
      "Speed: 2.0ms preprocess, 92.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\n",
      "0: 640x352 1 person, 82.6ms\n",
      "Speed: 2.0ms preprocess, 82.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\n",
      "0: 640x352 1 person, 80.6ms\n",
      "Speed: 2.0ms preprocess, 80.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\n",
      "0: 640x352 1 person, 81.6ms\n",
      "Speed: 3.0ms preprocess, 81.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\n",
      "0: 640x352 1 person, 87.5ms\n",
      "Speed: 3.0ms preprocess, 87.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\n",
      "0: 640x352 1 person, 81.5ms\n",
      "Speed: 2.0ms preprocess, 81.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\n",
      "0: 640x352 1 person, 80.5ms\n",
      "Speed: 2.0ms preprocess, 80.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\n",
      "0: 640x352 1 person, 87.5ms\n",
      "Speed: 2.0ms preprocess, 87.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\n",
      "0: 640x352 1 person, 85.0ms\n",
      "Speed: 2.0ms preprocess, 85.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\n",
      "0: 640x352 1 person, 90.5ms\n",
      "Speed: 2.0ms preprocess, 90.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\n",
      "0: 640x352 1 person, 84.5ms\n",
      "Speed: 3.0ms preprocess, 84.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\n",
      "0: 640x352 1 person, 82.5ms\n",
      "Speed: 2.0ms preprocess, 82.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\n",
      "0: 640x352 1 person, 87.6ms\n",
      "Speed: 2.0ms preprocess, 87.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\n",
      "0: 640x352 1 person, 81.6ms\n",
      "Speed: 2.0ms preprocess, 81.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\n",
      "0: 640x352 1 person, 85.6ms\n",
      "Speed: 2.0ms preprocess, 85.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\n",
      "0: 640x352 1 person, 106.6ms\n",
      "Speed: 2.0ms preprocess, 106.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\n",
      "0: 640x352 1 person, 87.6ms\n",
      "Speed: 2.0ms preprocess, 87.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\n",
      "0: 640x352 1 person, 90.6ms\n",
      "Speed: 3.0ms preprocess, 90.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\n",
      "0: 640x352 1 person, 90.6ms\n",
      "Speed: 2.0ms preprocess, 90.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\n",
      "0: 640x352 1 person, 75.5ms\n",
      "Speed: 2.0ms preprocess, 75.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\n",
      "0: 640x352 1 person, 80.5ms\n",
      "Speed: 2.0ms preprocess, 80.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\n",
      "0: 640x352 1 person, 82.6ms\n",
      "Speed: 3.0ms preprocess, 82.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\n",
      "0: 640x352 1 person, 82.6ms\n",
      "Speed: 2.0ms preprocess, 82.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\n",
      "0: 640x352 1 person, 85.6ms\n",
      "Speed: 2.0ms preprocess, 85.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\n",
      "0: 640x352 1 person, 78.6ms\n",
      "Speed: 2.0ms preprocess, 78.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "\n",
      "0: 640x352 1 person, 79.6ms\n",
      "Speed: 2.0ms preprocess, 79.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\n",
      "0: 640x352 1 person, 76.5ms\n",
      "Speed: 2.0ms preprocess, 76.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\n",
      "0: 640x352 1 person, 76.5ms\n",
      "Speed: 2.0ms preprocess, 76.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\n",
      "0: 640x352 1 person, 87.6ms\n",
      "Speed: 2.0ms preprocess, 87.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\n",
      "0: 640x352 1 person, 76.6ms\n",
      "Speed: 3.0ms preprocess, 76.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\n",
      "0: 640x352 1 person, 75.5ms\n",
      "Speed: 2.0ms preprocess, 75.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\n",
      "0: 640x352 1 person, 80.5ms\n",
      "Speed: 2.0ms preprocess, 80.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\n",
      "0: 640x352 1 person, 76.5ms\n",
      "Speed: 3.0ms preprocess, 76.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\n",
      "0: 640x352 1 person, 79.6ms\n",
      "Speed: 2.0ms preprocess, 79.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\n",
      "0: 640x352 1 person, 95.1ms\n",
      "Speed: 2.0ms preprocess, 95.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\n",
      "0: 640x352 1 person, 85.6ms\n",
      "Speed: 2.0ms preprocess, 85.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\n",
      "0: 640x352 1 person, 88.6ms\n",
      "Speed: 3.0ms preprocess, 88.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import load_model\n",
    "from ultralytics import YOLO\n",
    "\n",
    "\n",
    "\n",
    "def calculate_angle(a, b, c):\n",
    "    a, b, c = np.array(a), np.array(b), np.array(c)\n",
    "    ba = a - b\n",
    "    bc = c - b\n",
    "    cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n",
    "    angle = np.arccos(np.clip(cosine_angle, -1.0, 1.0))\n",
    "    return np.degrees(angle)\n",
    "\n",
    "def calculate_distance(a, b):\n",
    "    return np.linalg.norm(np.array(a) - np.array(b))\n",
    "\n",
    "def extract_pose_features(video_path, model, scaler_X, encoder, modelYolo):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        alto_original, ancho_original = frame.shape[:2]\n",
    "        nuevo_alto = 1100  # Nueva altura deseada\n",
    "        nuevo_ancho = int((nuevo_alto / alto_original) * ancho_original)\n",
    "        frame = cv2.resize(frame, (nuevo_ancho, nuevo_alto))\n",
    "\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = modelYolo(frame)\n",
    "\n",
    "        keypoints = results[0].keypoints.data.numpy().tolist()[0]\n",
    "        \n",
    "        # Definir las conexiones entre keypoints\n",
    "        connections = [(5, 6), (5, 7), (6, 8), (7, 9), (8, 10), (11, 12), (5, 11), (6, 12), (11, 13), (12, 14), (13, 15), (14, 16)]\n",
    "\n",
    "        for connection in connections:\n",
    "            start_keypoint = connection[0]\n",
    "            end_keypoint = connection[1]\n",
    "\n",
    "            # Obtener coordenadas del punto inicial y final de la conexión\n",
    "            start_point = (int(keypoints[start_keypoint][0]), int(keypoints[start_keypoint][1]))\n",
    "            end_point = (int(keypoints[end_keypoint][0]), int(keypoints[end_keypoint][1]))\n",
    "\n",
    "            # Dibujar línea entre los puntos\n",
    "            cv2.line(frame, start_point, end_point, (255, 0, 0), 2)\n",
    "\n",
    "            # Dibujar puntos de inicio y fin de la conexión\n",
    "            cv2.circle(frame, start_point, 5, (0, 255, 0), -1)\n",
    "            cv2.circle(frame, end_point, 5, (0, 255, 0), -1)\n",
    "\n",
    "        if len(keypoints) == 17:\n",
    "\n",
    "\n",
    "            # Extract relevant angles and distances\n",
    "            left_shoulder = keypoints[5]\n",
    "            right_shoulder = keypoints[6]\n",
    "            left_elbow = keypoints[7]\n",
    "            right_elbow = keypoints[8]\n",
    "            left_wrist = keypoints[9]\n",
    "            right_wrist = keypoints[10]\n",
    "            left_hip = keypoints[11]\n",
    "            right_hip = keypoints[12]\n",
    "            left_knee = keypoints[13]\n",
    "            right_knee = keypoints[14]\n",
    "            left_ankle = keypoints[15]\n",
    "            right_ankle = keypoints[16]\n",
    "\n",
    "            # Example angles\n",
    "            left_elbow_angle = calculate_angle(left_shoulder, left_elbow, left_wrist)\n",
    "            right_elbow_angle = calculate_angle(right_shoulder, right_elbow, right_wrist)\n",
    "            left_shoulder_angle = calculate_angle(left_hip, left_shoulder, left_elbow)\n",
    "            right_shoulder_angle = calculate_angle(right_hip, right_shoulder, right_elbow)\n",
    "            left_knee_angle = calculate_angle(left_hip, left_knee, left_ankle)\n",
    "            right_knee_angle = calculate_angle(right_hip, right_knee, right_ankle)\n",
    "            \n",
    "            # Example distances\n",
    "            wrist_distance = calculate_distance(left_wrist, right_wrist)\n",
    "            ankle_distance = calculate_distance(left_ankle, right_ankle)\n",
    "            shoulder_distance = calculate_distance(left_shoulder, right_shoulder)\n",
    "            hip_distance = calculate_distance(left_hip, right_hip)\n",
    "\n",
    "            features = np.array(keypoints).flatten().tolist()\n",
    "\n",
    "            X = scaler_X.transform(np.array(features).reshape(1, -1))\n",
    "            predictions = model.predict(X)\n",
    "\n",
    "            # Separar las predicciones\n",
    "            predicciones_continuas = predictions[0]\n",
    "            prediccion_clase = encoder.inverse_transform(np.argmax(predictions[1], axis=1))\n",
    "\n",
    "            features_video = [left_elbow_angle, right_elbow_angle, left_shoulder_angle, right_shoulder_angle, left_knee_angle, right_knee_angle, wrist_distance, ankle_distance, shoulder_distance, hip_distance]\n",
    "\n",
    "            # Comparar características del video con las predichas\n",
    "            porcentaje_diferencia = 0.15  # Porcentaje de diferencia permitido\n",
    "            errores = np.where(np.abs(np.array(features_video) - predicciones_continuas) > porcentaje_diferencia)[0]\n",
    "            errores = list(set(errores))\n",
    "\n",
    "            caracteristicas = [\"Angulo del codo izquierdo\", \"Angulo del codo derecho\", \"Angulo del hombro izquierdo\", \"Angulo del hombro derecho\", \"Angulo de la rodilla izquierda\", \"Angulo de la rodilla derecha\", \"Distancia entre munecas\", \"Distancia entre tobillos\", \"Distancia entre hombros\", \"Distancia entre caderas\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            clase_predicha = prediccion_clase[0]\n",
    "            porcentaje_prediccion = np.max(predictions[1]) * 100\n",
    "\n",
    "            # Mostrar la predicción de clase\n",
    "            texto_clasificacion = f'Clasificacion: {clase_predicha} - Probabilidad: {porcentaje_prediccion:.2f}%'\n",
    "            cv2.putText(frame, texto_clasificacion, (50, 220), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n",
    "\n",
    "            if len(errores) > 0:\n",
    "                for i, error in enumerate(errores):\n",
    "                    desplazamiento_vertical = 240 + (i * 30)  # Incrementa la posición vertical para cada error adicional\n",
    "                    cv2.putText(frame, f'Error en : {caracteristicas[error]}', (50, desplazamiento_vertical), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "\n",
    "            \n",
    "\n",
    "        cv2.imshow('Frame', cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "        # Listen for 'q' key to quit\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "# Cargar el modelo y las clases\n",
    "ruta_actual = os.getcwd()\n",
    "carpeta_Yolo= os.path.dirname(ruta_actual)\n",
    "ruta_raiz = os.path.dirname(carpeta_Yolo)\n",
    "\n",
    "ruta_dataset = os.path.join(ruta_raiz, 'Data Set') \n",
    "ruta_video = os.path.join(ruta_dataset, 'test_video.mp4')\n",
    "ruta_npy = os.path.join(ruta_actual, 'datos', 'classes.npy')\n",
    "ruta_scaler = os.path.join(ruta_actual,'datos','scaler_X.pkl')\n",
    "ruta_modelo = os.path.join(ruta_actual, 'model', 'exercise_model.h5')\n",
    "\n",
    "modelYolo = YOLO(\"yolov8n-pose.pt\")\n",
    "\n",
    "# Cargar el modelo y scaler\n",
    "model = load_model(ruta_modelo)\n",
    "scaler_X = joblib.load(ruta_scaler)\n",
    "\n",
    "classes = np.load(ruta_npy, allow_pickle=True)\n",
    "encoder = LabelEncoder()\n",
    "encoder.classes_ = classes\n",
    "\n",
    "# Realizar la predicción\n",
    "extract_pose_features(ruta_video,model,scaler_X,encoder,modelYolo)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Unir_TFM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
